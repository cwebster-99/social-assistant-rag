{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG exploration\n",
    "\n",
    "![goal_post](goal_post.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Social assistant, with **off the shelf model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://microsoftfabric.devpost.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Fetch the content from the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the content of the blog post\n",
    "blog_post_content = soup.get_text()\n",
    "\n",
    "# Convert the content to JSON\n",
    "content = json.dumps({\"content\": blog_post_content})\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages to give LLM, to create a short LinkedIn post based on a blog post\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a social assistant who writes creative content. You will politely decline any other requests from the user not related to creating content. Don't talk about a single VS Code release and don't talk about release dates at all. Instead, only talk about the relevant features. Don't include made up links, but do provide real links to the VS Code release notes for specific features. You format all your responses as Markdown unless otherwise specified. Avoid wrapping your entire response in a markdown code element.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": f\"Create a very short LinkedIn post using the following: {content}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Text search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract key topics & features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages to give LLM, to extract key topics & features\n",
    "\n",
    "topic_system_message = \"\"\"\n",
    "You are an expert at conducting entity extraction. Generate top topics and functionality based on provided content. Focus on identifying key concepts, themes, and relevant terms related to specific developer tooling, with a particular emphasis on VS Code features. Make sure entities you extract are directly relevant to the developer environment described. Don't mention specific dates or years. Use advanced search techniques, including Boolean operators and keyword variations, to craft precise, optimized queries that yield the most relevant results. Aim for clarity, relevance, and depth to cover all aspects of the topic efficiently. Simply list the phrases without additional explanation or details. Do not list any bullet points or numbered lists or quotation marks.\n",
    "\"\"\"\n",
    "\n",
    "topic_user_message=\"Come up with a list of top 5 developer tooling topics, functionalities, and relevant terms, with a strong focus on VS Code features and integrations based on the following content: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_topics(content, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": topic_system_message},\n",
    "        {\"role\": \"user\", \"content\": topic_user_message+content}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    key_topics = response.choices[0].message.content.split('\\n')\n",
    "    return key_topics\n",
    "\n",
    "key_topics = extract_key_topics(content)\n",
    "print(\"\\n\".join([topic + \"\\n\" for topic in key_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load split_docs_contents.json as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform text search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¢ Semantic reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages to give LLM, to re-rank the documents based on semantic relevance\n",
    "\n",
    "rerank_system_message = \"\"\"\n",
    "You are tasked with re-ranking a set of documents based on their relevance to given search queries. The documents have already been retrieved based on initial search criteria, but your role is to refine the ranking by considering factors such as semantic similarity to the query, context relevance, and alignment with the user's intent. Focus on documents that provide concise, high-quality information, ensuring that the top-ranked documents answer the query as accurately and completely as possible. If you can't rank them based on semantic relevance, give higher rank to documents with VS Code features that were published most recently. Make sure to return the full name of the feature and URL of each release note document, and format your response as a Markdown list item, with the URL in parentheses. Do not include any additional information or commentary about the documents. List a variety of documents, and give more weight to documents that mention Python and or notebooks features. Only return the top 3 documents and reference them by the feature name, not the release version or date.\n",
    "\"\"\"\n",
    "\n",
    "rerank_user_message=f\"Here are some documents: {top_10_docs.to_json(orient='records')}. Re-rank those documents based on these key VS Code functionalities: {key_topics}. Only return the top 3 documents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents(model=\"gpt-4o-mini\"):\n",
    "    # Truncate the user message to fit within the token limit\n",
    "    max_length = 7500  # Adjust this value as needed to fit within the token limit\n",
    "    truncated_user_message = rerank_user_message[:max_length]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rerank_system_message},\n",
    "        {\"role\": \"user\", \"content\": truncated_user_message}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    reranked_documents = response.choices[0].message.content.split('\\n')\n",
    "    return reranked_documents\n",
    "\n",
    "reranked_documents = rerank_documents()\n",
    "print(\"\\n\".join([doc + \"\\n\" for doc in reranked_documents]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Social assistant, with **relevant features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_answer(content, context, completion_model):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\":  f\"Create a very short LinkedIn post using the following content: {content}. Also, include the following established VS Code features along with their URLs in your response, so folks seeing the post can try them out: {context}.\"}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions(\n",
    "        model=completion_model,\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "print(generate_llm_answer(content, reranked_documents, completion_model=\"gpt-4o-mini\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare responses between chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_llm_answer(content, reranked_documents, completion_model=\"Mistral-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_llm_answer(content, reranked_documents, completion_model=\"meta-llama-3-8b-instruct\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it out yourself! ðŸ˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VS Code extensions used for this demo:\n",
    "\n",
    "![extensions](vsc_extensions.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
